{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.85042898 -1.30203212 -0.09808326 ... -0.99946244 -0.30188952\n",
      " -1.45009041]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/crc.nd.edu/user/p/pbui/pub/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/afs/crc.nd.edu/user/p/pbui/pub/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      yyyymm     Index      D12       E12       b/m     tbl     AAA     BAA  \\\n",
      "0     192701     13.21   0.6967    1.2290  0.443706  0.0323  0.0466  0.0561   \n",
      "1     192702     13.84   0.7033    1.2180  0.428501  0.0329  0.0467  0.0559   \n",
      "2     192703     13.93   0.7100    1.2080  0.469765  0.0320  0.0462  0.0554   \n",
      "3     192704     14.17   0.7167    1.1970  0.456754  0.0339  0.0458  0.0548   \n",
      "4     192705     14.91   0.7233    1.1860  0.434783  0.0333  0.0457  0.0550   \n",
      "5     192706     14.77   0.7300    1.1750  0.452385  0.0307  0.0458  0.0555   \n",
      "6     192707     15.73   0.7367    1.1640  0.414553  0.0296  0.0460  0.0555   \n",
      "7     192708     16.43   0.7433    1.1530  0.396227  0.0270  0.0456  0.0548   \n",
      "8     192709     17.14   0.7500    1.1430  0.380586  0.0268  0.0454  0.0542   \n",
      "9     192710     16.23   0.7567    1.1320  0.413801  0.0308  0.0451  0.0538   \n",
      "10    192711     17.33   0.7633    1.1210  0.379396  0.0304  0.0449  0.0535   \n",
      "11    192712     17.66   0.7700    1.1100  0.374689  0.0317  0.0446  0.0532   \n",
      "12    192801     17.57   0.7767    1.1330  0.378670  0.0331  0.0446  0.0535   \n",
      "13    192802     17.26   0.7833    1.1550  0.386077  0.0333  0.0446  0.0533   \n",
      "14    192803     19.13   0.7900    1.1770  0.363255  0.0327  0.0446  0.0532   \n",
      "15    192804     19.75   0.7967    1.2000  0.368095  0.0362  0.0446  0.0533   \n",
      "16    192805     20.00   0.8033    1.2220  0.354397  0.0390  0.0449  0.0542   \n",
      "17    192806     19.19   0.8100    1.2450  0.370300  0.0392  0.0457  0.0555   \n",
      "18    192807     19.43   0.8167    1.2680  0.360648  0.0412  0.0461  0.0558   \n",
      "19    192808     20.87   0.8233    1.2900  0.324030  0.0436  0.0464  0.0561   \n",
      "20    192809     21.37   0.8300    1.3120  0.328166  0.0457  0.0461  0.0559   \n",
      "21    192810     21.68   0.8367    1.3350  0.308931  0.0470  0.0461  0.0558   \n",
      "22    192811     24.28   0.8433    1.3570  0.265526  0.0426  0.0458  0.0555   \n",
      "23    192812     24.35   0.8500    1.3800  0.259667  0.0426  0.0461  0.0560   \n",
      "24    192901     25.74   0.8600    1.3990  0.245347  0.0466  0.0462  0.0563   \n",
      "25    192902     25.59   0.8700    1.4180  0.245424  0.0439  0.0466  0.0566   \n",
      "26    192903     25.53   0.8800    1.4380  0.272300  0.0460  0.0470  0.0579   \n",
      "27    192904     25.94   0.8900    1.4570  0.263397  0.0480  0.0469  0.0580   \n",
      "28    192905     24.83   0.9000    1.4760  0.282775  0.0509  0.0470  0.0580   \n",
      "29    192906     27.62   0.9100    1.4950  0.253581  0.0480  0.0477  0.0594   \n",
      "...      ...       ...      ...       ...       ...     ...     ...     ...   \n",
      "1062  201507  2,103.84  41.9981   93.4933  0.308953  0.0003  0.0415  0.0520   \n",
      "1063  201508  1,972.18  42.2543   92.0767  0.330671  0.0007  0.0404  0.0519   \n",
      "1064  201509  1,920.03  42.5104   90.6600  0.335612  0.0002  0.0407  0.0534   \n",
      "1065  201510  2,079.36  42.8029   89.2833  0.309414  0.0002  0.0395  0.0534   \n",
      "1066  201511  2,080.41  43.0954   87.9067  0.308429  0.0012  0.0406  0.0546   \n",
      "1067  201512  2,043.94  43.3879   86.5300  0.313649  0.0023  0.0397  0.0546   \n",
      "1068  201601  1,940.24  43.5506   86.5000  0.331911  0.0026  0.0400  0.0545   \n",
      "1069  201602  1,932.23  43.7133   86.4700  0.330902  0.0031  0.0396  0.0534   \n",
      "1070  201603  2,059.74  43.8759   86.4400  0.327955  0.0029  0.0382  0.0513   \n",
      "1071  201604  2,065.30  44.0706   86.6000  0.326321  0.0023  0.0362  0.0479   \n",
      "1072  201605  2,096.95  44.2652   86.7600  0.326072  0.0027  0.0365  0.0468   \n",
      "1073  201606  2,098.86  44.4599   86.9200  0.323475  0.0027  0.0350  0.0453   \n",
      "1074  201607  2,173.60  44.6485   87.6433  0.314661  0.0030  0.0328  0.0422   \n",
      "1075  201608  2,170.95  44.8371   88.3667  0.315197  0.0030  0.0332  0.0424   \n",
      "1076  201609  2,168.27  45.0257   89.0900  0.316794  0.0029  0.0341  0.0431   \n",
      "1077  201610  2,126.15  45.2507   90.9100  0.319688  0.0033  0.0351  0.0438   \n",
      "1078  201611  2,198.81  45.4756   92.7300  0.303286  0.0045  0.0386  0.0471   \n",
      "1079  201612  2,238.83  45.7006   94.5500  0.293479  0.0051  0.0406  0.0483   \n",
      "1080  201701  2,278.87  45.9279   96.4633  0.291980  0.0051  0.0392  0.0466   \n",
      "1081  201702  2,363.64  46.1552   98.3767  0.278678  0.0052  0.0395  0.0464   \n",
      "1082  201703  2,362.72  46.3824  100.2900  0.281599  0.0074  0.0401  0.0468   \n",
      "1083  201704  2,384.20  46.6613  101.5333  0.277870  0.0080  0.0387  0.0457   \n",
      "1084  201705  2,411.80  46.9402  102.7767  0.276969  0.0089  0.0385  0.0455   \n",
      "1085  201706  2,423.41  47.2190  104.0200  0.272545  0.0098  0.0368  0.0437   \n",
      "1086  201707  2,470.30  47.5370  105.0400  0.265804  0.0107  0.0370  0.0439   \n",
      "1087  201708  2,471.65  47.8551  106.0600  0.265114  0.0101  0.0363  0.0431   \n",
      "1088  201709  2,519.36  48.1731  107.0800  0.259706  0.0103  0.0363  0.0430   \n",
      "1089  201710  2,575.26  48.4261  108.0133  0.248906  0.0107  0.0360  0.0432   \n",
      "1090  201711  2,584.84  48.6790  108.9467  0.239727  0.0123  0.0357  0.0427   \n",
      "1091  201712  2,673.61  48.9320  109.8800  0.235393  0.0132  0.0351  0.0422   \n",
      "\n",
      "         lty      ntis    ...      Index - 15  Index - 16  Index - 17  \\\n",
      "0     0.0351  0.050834    ...             NaN         NaN         NaN   \n",
      "1     0.0347  0.051682    ...             NaN         NaN         NaN   \n",
      "2     0.0331  0.046370    ...             NaN         NaN         NaN   \n",
      "3     0.0333  0.050518    ...             NaN         NaN         NaN   \n",
      "4     0.0327  0.055279    ...             NaN         NaN         NaN   \n",
      "5     0.0334  0.058826    ...             NaN         NaN         NaN   \n",
      "6     0.0333  0.059754    ...             NaN         NaN         NaN   \n",
      "7     0.0329  0.054526    ...             NaN         NaN         NaN   \n",
      "8     0.0330  0.094617    ...             NaN         NaN         NaN   \n",
      "9     0.0325  0.094370    ...             NaN         NaN         NaN   \n",
      "10    0.0320  0.082270    ...             NaN         NaN         NaN   \n",
      "11    0.0316  0.076474    ...             NaN         NaN         NaN   \n",
      "12    0.0321  0.062605    ...             NaN         NaN         NaN   \n",
      "13    0.0318  0.055172    ...             NaN         NaN         NaN   \n",
      "14    0.0317  0.054364    ...             NaN         NaN         NaN   \n",
      "15    0.0319  0.049372    ...             NaN         NaN         NaN   \n",
      "16    0.0327  0.047187    ...             NaN         NaN         NaN   \n",
      "17    0.0326  0.050298    ...             NaN         NaN         NaN   \n",
      "18    0.0344  0.059380    ...             NaN         NaN         NaN   \n",
      "19    0.0341  0.057398    ...             NaN         NaN         NaN   \n",
      "20    0.0346  0.027979    ...             NaN         NaN         NaN   \n",
      "21    0.0336  0.034018    ...             NaN         NaN         NaN   \n",
      "22    0.0338  0.038372    ...             NaN         NaN         NaN   \n",
      "23    0.0340  0.063068    ...             NaN         NaN         NaN   \n",
      "24    0.0349  0.078448    ...           16.23       17.14       16.43   \n",
      "25    0.0363  0.071782    ...           17.33       16.23       17.14   \n",
      "26    0.0377  0.079803    ...           17.66       17.33       16.23   \n",
      "27    0.0358  0.099320    ...           17.57       17.66       17.33   \n",
      "28    0.0373  0.117985    ...           17.26       17.57       17.66   \n",
      "29    0.0367  0.116196    ...           19.13       17.26       17.57   \n",
      "...      ...       ...    ...             ...         ...         ...   \n",
      "1062  0.0263 -0.008070    ...        1,883.95    1,872.34    1,859.45   \n",
      "1063  0.0264 -0.009535    ...        1,923.57    1,883.95    1,872.34   \n",
      "1064  0.0253 -0.012923    ...        1,960.23    1,923.57    1,883.95   \n",
      "1065  0.0259 -0.016208    ...        1,930.67    1,960.23    1,923.57   \n",
      "1066  0.0265 -0.017810    ...        2,003.37    1,930.67    1,960.23   \n",
      "1067  0.0268 -0.021611    ...        1,927.29    2,003.37    1,930.67   \n",
      "1068  0.0236 -0.020262    ...        2,018.05    1,927.29    2,003.37   \n",
      "1069  0.0217 -0.024023    ...        2,067.56    2,018.05    1,927.29   \n",
      "1070  0.0218 -0.022999    ...        2,058.90    2,067.56    2,018.05   \n",
      "1071  0.0223 -0.023554    ...        1,994.99    2,058.90    2,067.56   \n",
      "1072  0.0219 -0.027005    ...        2,104.50    1,994.99    2,058.90   \n",
      "1073  0.0179 -0.028683    ...        2,067.89    2,104.50    1,994.99   \n",
      "1074  0.0175 -0.031666    ...        2,085.51    2,067.89    2,104.50   \n",
      "1075  0.0186 -0.030725    ...        2,107.39    2,085.51    2,067.89   \n",
      "1076  0.0196 -0.032610    ...        2,063.11    2,107.39    2,085.51   \n",
      "1077  0.0220 -0.028997    ...        2,103.84    2,063.11    2,107.39   \n",
      "1078  0.0267 -0.027361    ...        1,972.18    2,103.84    2,063.11   \n",
      "1079  0.0272 -0.025012    ...        1,920.03    1,972.18    2,103.84   \n",
      "1080  0.0278 -0.022562    ...        2,079.36    1,920.03    1,972.18   \n",
      "1081  0.0270 -0.018621    ...        2,080.41    2,079.36    1,920.03   \n",
      "1082  0.0274 -0.016151    ...        2,043.94    2,080.41    2,079.36   \n",
      "1083  0.0265 -0.015497    ...        1,940.24    2,043.94    2,080.41   \n",
      "1084  0.0256 -0.010100    ...        1,932.23    1,940.24    2,043.94   \n",
      "1085  0.0258 -0.009702    ...        2,059.74    1,932.23    1,940.24   \n",
      "1086  0.0262 -0.013104    ...        2,065.30    2,059.74    1,932.23   \n",
      "1087  0.0242 -0.012138    ...        2,096.95    2,065.30    2,059.74   \n",
      "1088  0.0259 -0.011027    ...        2,098.86    2,096.95    2,065.30   \n",
      "1089  0.0261 -0.012358    ...        2,173.60    2,098.86    2,096.95   \n",
      "1090  0.0260 -0.012243    ...        2,170.95    2,173.60    2,098.86   \n",
      "1091  0.0254 -0.019946    ...        2,168.27    2,170.95    2,173.60   \n",
      "\n",
      "      Index - 18  Index - 19  Index - 20  Index - 21  Index - 22 Index - 23  \\\n",
      "0            NaN         NaN         NaN         NaN         NaN        NaN   \n",
      "1            NaN         NaN         NaN         NaN         NaN        NaN   \n",
      "2            NaN         NaN         NaN         NaN         NaN        NaN   \n",
      "3            NaN         NaN         NaN         NaN         NaN        NaN   \n",
      "4            NaN         NaN         NaN         NaN         NaN        NaN   \n",
      "5            NaN         NaN         NaN         NaN         NaN        NaN   \n",
      "6            NaN         NaN         NaN         NaN         NaN        NaN   \n",
      "7            NaN         NaN         NaN         NaN         NaN        NaN   \n",
      "8            NaN         NaN         NaN         NaN         NaN        NaN   \n",
      "9            NaN         NaN         NaN         NaN         NaN        NaN   \n",
      "10           NaN         NaN         NaN         NaN         NaN        NaN   \n",
      "11           NaN         NaN         NaN         NaN         NaN        NaN   \n",
      "12           NaN         NaN         NaN         NaN         NaN        NaN   \n",
      "13           NaN         NaN         NaN         NaN         NaN        NaN   \n",
      "14           NaN         NaN         NaN         NaN         NaN        NaN   \n",
      "15           NaN         NaN         NaN         NaN         NaN        NaN   \n",
      "16           NaN         NaN         NaN         NaN         NaN        NaN   \n",
      "17           NaN         NaN         NaN         NaN         NaN        NaN   \n",
      "18           NaN         NaN         NaN         NaN         NaN        NaN   \n",
      "19           NaN         NaN         NaN         NaN         NaN        NaN   \n",
      "20           NaN         NaN         NaN         NaN         NaN        NaN   \n",
      "21           NaN         NaN         NaN         NaN         NaN        NaN   \n",
      "22           NaN         NaN         NaN         NaN         NaN        NaN   \n",
      "23           NaN         NaN         NaN         NaN         NaN        NaN   \n",
      "24         15.73       14.77       14.91       14.17       13.93      13.84   \n",
      "25         16.43       15.73       14.77       14.91       14.17      13.93   \n",
      "26         17.14       16.43       15.73       14.77       14.91      14.17   \n",
      "27         16.23       17.14       16.43       15.73       14.77      14.91   \n",
      "28         17.33       16.23       17.14       16.43       15.73      14.77   \n",
      "29         17.66       17.33       16.23       17.14       16.43      15.73   \n",
      "...          ...         ...         ...         ...         ...        ...   \n",
      "1062    1,782.59    1,848.36    1,805.81    1,756.54    1,681.55   1,632.97   \n",
      "1063    1,859.45    1,782.59    1,848.36    1,805.81    1,756.54   1,681.55   \n",
      "1064    1,872.34    1,859.45    1,782.59    1,848.36    1,805.81   1,756.54   \n",
      "1065    1,883.95    1,872.34    1,859.45    1,782.59    1,848.36   1,805.81   \n",
      "1066    1,923.57    1,883.95    1,872.34    1,859.45    1,782.59   1,848.36   \n",
      "1067    1,960.23    1,923.57    1,883.95    1,872.34    1,859.45   1,782.59   \n",
      "1068    1,930.67    1,960.23    1,923.57    1,883.95    1,872.34   1,859.45   \n",
      "1069    2,003.37    1,930.67    1,960.23    1,923.57    1,883.95   1,872.34   \n",
      "1070    1,927.29    2,003.37    1,930.67    1,960.23    1,923.57   1,883.95   \n",
      "1071    2,018.05    1,927.29    2,003.37    1,930.67    1,960.23   1,923.57   \n",
      "1072    2,067.56    2,018.05    1,927.29    2,003.37    1,930.67   1,960.23   \n",
      "1073    2,058.90    2,067.56    2,018.05    1,927.29    2,003.37   1,930.67   \n",
      "1074    1,994.99    2,058.90    2,067.56    2,018.05    1,927.29   2,003.37   \n",
      "1075    2,104.50    1,994.99    2,058.90    2,067.56    2,018.05   1,927.29   \n",
      "1076    2,067.89    2,104.50    1,994.99    2,058.90    2,067.56   2,018.05   \n",
      "1077    2,085.51    2,067.89    2,104.50    1,994.99    2,058.90   2,067.56   \n",
      "1078    2,107.39    2,085.51    2,067.89    2,104.50    1,994.99   2,058.90   \n",
      "1079    2,063.11    2,107.39    2,085.51    2,067.89    2,104.50   1,994.99   \n",
      "1080    2,103.84    2,063.11    2,107.39    2,085.51    2,067.89   2,104.50   \n",
      "1081    1,972.18    2,103.84    2,063.11    2,107.39    2,085.51   2,067.89   \n",
      "1082    1,920.03    1,972.18    2,103.84    2,063.11    2,107.39   2,085.51   \n",
      "1083    2,079.36    1,920.03    1,972.18    2,103.84    2,063.11   2,107.39   \n",
      "1084    2,080.41    2,079.36    1,920.03    1,972.18    2,103.84   2,063.11   \n",
      "1085    2,043.94    2,080.41    2,079.36    1,920.03    1,972.18   2,103.84   \n",
      "1086    1,940.24    2,043.94    2,080.41    2,079.36    1,920.03   1,972.18   \n",
      "1087    1,932.23    1,940.24    2,043.94    2,080.41    2,079.36   1,920.03   \n",
      "1088    2,059.74    1,932.23    1,940.24    2,043.94    2,080.41   2,079.36   \n",
      "1089    2,065.30    2,059.74    1,932.23    1,940.24    2,043.94   2,080.41   \n",
      "1090    2,096.95    2,065.30    2,059.74    1,932.23    1,940.24   2,043.94   \n",
      "1091    2,098.86    2,096.95    2,065.30    2,059.74    1,932.23   1,940.24   \n",
      "\n",
      "     Index - 24  \n",
      "0           NaN  \n",
      "1             1  \n",
      "2           NaN  \n",
      "3           NaN  \n",
      "4           NaN  \n",
      "5           NaN  \n",
      "6           NaN  \n",
      "7           NaN  \n",
      "8           NaN  \n",
      "9           NaN  \n",
      "10          NaN  \n",
      "11          NaN  \n",
      "12          NaN  \n",
      "13          NaN  \n",
      "14          NaN  \n",
      "15          NaN  \n",
      "16          NaN  \n",
      "17          NaN  \n",
      "18          NaN  \n",
      "19          NaN  \n",
      "20          NaN  \n",
      "21          NaN  \n",
      "22          NaN  \n",
      "23          NaN  \n",
      "24        13.21  \n",
      "25        13.84  \n",
      "26        13.93  \n",
      "27        14.17  \n",
      "28        14.91  \n",
      "29        14.77  \n",
      "...         ...  \n",
      "1062   1,685.73  \n",
      "1063   1,632.97  \n",
      "1064   1,681.55  \n",
      "1065   1,756.54  \n",
      "1066   1,805.81  \n",
      "1067   1,848.36  \n",
      "1068   1,782.59  \n",
      "1069   1,859.45  \n",
      "1070   1,872.34  \n",
      "1071   1,883.95  \n",
      "1072   1,923.57  \n",
      "1073   1,960.23  \n",
      "1074   1,930.67  \n",
      "1075   2,003.37  \n",
      "1076   1,927.29  \n",
      "1077   2,018.05  \n",
      "1078   2,067.56  \n",
      "1079   2,058.90  \n",
      "1080   1,994.99  \n",
      "1081   2,104.50  \n",
      "1082   2,067.89  \n",
      "1083   2,085.51  \n",
      "1084   2,107.39  \n",
      "1085   2,063.11  \n",
      "1086   2,103.84  \n",
      "1087   1,972.18  \n",
      "1088   1,920.03  \n",
      "1089   2,079.36  \n",
      "1090   2,080.41  \n",
      "1091   2,043.94  \n",
      "\n",
      "[1092 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv(\"raw_predictors_until2013.csv\")\n",
    "sLength = len(data['yyyymm'])\n",
    "# create new columns\n",
    "for x in range(1,25):\n",
    "    t = str(x)\n",
    "    data['Index - '+t] = np.nan \n",
    "print(np.random.randn(sLength))\n",
    "\n",
    "# add correct values to columns\n",
    "for z in range(1,25):\n",
    "    l = []\n",
    "    t = str(z)\n",
    "    for y in range(24,sLength):\n",
    "        hold = data['Index'][y-z]\n",
    "        l.append(hold)\n",
    "    data['Index - '+t][24:] = l\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.21\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4ef8ef8654fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msLength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'yyyymm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# add the bullMarket column with random ints (to be replaced in next cell)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bull'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msLength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create bullMarkert for ret_sp500+1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbull\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "sLength = len(data['yyyymm'])\n",
    "# add the bullMarket column with random ints (to be replaced in next cell)\n",
    "data['bull'] = pd.Series(np.random.randn(sLength), index=data.index)\n",
    "# Create bullMarkert for ret_sp500+1\n",
    "bull = []\n",
    "for data in data['CRSP_SPvw']:\n",
    "    if data > 0:\n",
    "        bull.append(1)\n",
    "    else:\n",
    "        bull.append(0)\n",
    "data['bull'] = bull\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14275, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Rename columns\n",
    "sp500.columns = ['Date', 'sp500', 'sp500+1','ret_sp500', 'ret_sp500+1']\n",
    "sp500 = sp500.dropna()\n",
    "sLength = len(sp500['Date'])\n",
    "# add the bullMarket column with random ints (to be replaced in next cell)\n",
    "sp500['bullMarket'] = pd.Series(np.random.randn(sLength), index=sp500.index)\n",
    "\n",
    "# Create bullMarkert for ret_sp500+1\n",
    "bull = []\n",
    "for data in sp500['ret_sp500+1']:\n",
    "    if data > 0:\n",
    "        bull.append(1)\n",
    "    else:\n",
    "        bull.append(0)\n",
    "sp500['bullMarket'] = bull\n",
    "# print(sp500)\n",
    "\n",
    "sp500.shape\n",
    "# print(sp500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata = pd.read_csv(\"Total_Data_no_dots.csv\",low_memory=False)\n",
    "\n",
    "result = pd.merge(sp500, tdata, on='Date', how='left')\n",
    "\n",
    "result  = result.dropna()\n",
    "\n",
    "# assume we want to predict the raw index of sp500 at T+1, given all the info at T\n",
    "\n",
    "y_reg = result['ret_sp500+1']\n",
    "\n",
    "y_cal = result['bullMarket']\n",
    " \n",
    "# list(result)\n",
    "\n",
    "result = result.drop(columns=['VOX Close', 'DGS1', 'DGS10', 'T10Y2Y', 'DBAA', 'USD1MTD156N', 'DCOILBRENTEU', 'DTWEXB', 'DEXUSEU', 'T10YIE', 'T5YIFR', 'BAMLH0A0HYM2EY'])\n",
    "\n",
    "x = result.drop(columns=['Date','sp500_x','sp500+1','ret_sp500+1','Unnamed: 0'])\n",
    "\n",
    "# print(result.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.8108592714839336\n",
      "MSE:  0.8220161493992437\n",
      "the main take way: the mean squared error is always within 1% \n"
     ]
    }
   ],
   "source": [
    "# Linear Regression Model\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "kf = KFold(x.shape[0], n_folds=2, shuffle=True)\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    x_train, x_test = x.iloc[train_index], x.iloc[test_index]\n",
    "    y_train, y_test = y_reg.iloc[train_index], y_reg.iloc[test_index]\n",
    "    y_train_cal, y_test_cal = y_cal.iloc[train_index], y_cal.iloc[test_index]\n",
    "\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = regr.predict(x_test)\n",
    "    print(\"MSE: \", mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "print( \"the main take way: the mean squared error is always within 1% \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2132    0]\n",
      " [   0 2242]]\n",
      "precision:  1.0\n",
      "recall:  1.0\n",
      "the main take way: 'perfect' prediction, overfitting problem\n"
     ]
    }
   ],
   "source": [
    "# clssification \n",
    "\n",
    "# decision tree CART algorithm\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(x_train, y_train_cal)\n",
    "predicted = clf.predict(x_test)\n",
    "# print (predicted)\n",
    "c_matrix = confusion_matrix(predicted,y_test_cal)\n",
    "precision = c_matrix[1,1]/(c_matrix[1,1]+c_matrix[0,1])\n",
    "recall= c_matrix[1,1]/(c_matrix[1,1]+c_matrix[1,0])\n",
    "\n",
    "print(c_matrix)  \n",
    "\n",
    "print(\"precision: \", precision)  \n",
    "print(\"recall: \", recall)  \n",
    "\n",
    "print( \"the main take way: 'perfect' prediction, overfitting problem\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 120  143]\n",
      " [2012 2099]]\n",
      "precision:  0.9362176628010704\n",
      "recall:  0.5105813670639747\n"
     ]
    }
   ],
   "source": [
    "# naive Bayes \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "gnb.fit(x_train, y_train_cal).predict(x_test)\n",
    "\n",
    "predicted= gnb.predict(x_test)\n",
    "\n",
    "c_matrix = confusion_matrix(predicted,y_test_cal)\n",
    "precision = c_matrix[1,1]/(c_matrix[1,1]+c_matrix[0,1])\n",
    "recall= c_matrix[1,1]/(c_matrix[1,1]+c_matrix[1,0])\n",
    "\n",
    "print(c_matrix)  \n",
    "\n",
    "print(\"precision: \", precision)  \n",
    "print(\"recall: \", recall)  \n",
    "\n",
    "print( \"the main take way: High Precision, recall is slightly better than a coin flip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1063    0]\n",
      " [1069 2242]]\n",
      "precision:  1.0\n",
      "recall:  0.6771368166717004\n",
      "the main take way: High Precision, recall is way more better than naive Bayes \n"
     ]
    }
   ],
   "source": [
    "# Support Vector Classification\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(gamma='auto')\n",
    "clf.fit(x_train, y_train_cal) \n",
    "\n",
    "predicted= clf.predict(x_test)\n",
    "\n",
    "c_matrix = confusion_matrix(predicted,y_test_cal)\n",
    "precision = c_matrix[1,1]/(c_matrix[1,1]+c_matrix[0,1])\n",
    "recall= c_matrix[1,1]/(c_matrix[1,1]+c_matrix[1,0])\n",
    "\n",
    "print(c_matrix)  \n",
    "\n",
    "print(\"precision: \", precision)  \n",
    "print(\"recall: \", recall)  \n",
    "\n",
    "print( \"the main take way: High Precision, recall is way more better than naive Bayes \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
